\documentclass[12pt, a4paper]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{parskip}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\lstdefinestyle{pystyle}{
    language=Python,
    basicstyle=\ttfamily\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{teal},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    frame=single,
    breaklines=true
}

\graphicspath{{./images/}}

\title{Week 4--5 Progress Report: Methods, Data, and Implementation}
\author{Farooq Mahmud}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\clearpage

%=============================================================================
\section{Project Goal and Research Question}
%=============================================================================

\subsection{Goal}

The goal of this capstone project is to develop and evaluate forecasting methods for daily average Uber trip duration in New York City. This capstone compares an ARIMA approach with an LSTM model on the same dataset \cite{nyctlc2024}. The aim is to produce a 14-day forecast from each method and to assess both using sMAPE and MASE metrics \cite{hyndman2006,prabhat2024}.

\subsection{Research Question}

How do ARIMA and LSTM compare for 14-day forecasting of daily average Uber trip duration in New York City when evaluated on sMAPE and MASE?

%=============================================================================
\section{Expanded Methods}
%=============================================================================

\subsection{Problem Definition}

This capstone predicts daily average Uber trip duration (in minutes) in New York City. The target variable is the mean trip duration per day, aggregated from high-volume for-hire vehicle (FHV) trip records \cite{nyctlc2024}. The inputs are a univariate time series of 366 (2024 was a leap year) daily observations. The outputs are (1) a 14-day point forecast of average trip duration from each method (ARIMA and LSTM), and (2) for comparison, the same horizon evaluated with sMAPE and MASE \cite{hyndman2006,prabhat2024} over a common 14-day period.

\subsection{Data Description}

The data are daily average Uber trip duration (in minutes) for New York City in 2024, derived from the New York City Taxi and Limousine Commission (TLC) High-Volume FHV trip record data \cite{nyctlc2024}. Key variables are \texttt{pickup\_date} and \texttt{avg\_duration\_min}. The preprocessed time-series has 366 daily observations. Preprocessing is done via PySpark aggregation to daily averages.

\subsection{Preprocessing Plan} \label{sec:preprocessing}

The following preprocessing is common to both modeling methods and produces the single daily time series (\texttt{pickup\_date}, \texttt{avg\_duration\_min}) that both ARIMA and LSTM use. It was applied to the NYC TLC High-Volume FHV trip record data \cite{nyctlc2024}. The PySpark code is given in Appendix~\ref{appendix:pyspark}. Preprocessing is necessary because the original dataset contains over 200 million trip records, of which about 179 million are Uber trip records. Outliers and invalid or nonsensical data are also removed resulting in a final total of about 159 million rows. These rows are converted to a CSV file representing the time-series. The CSV contains the 366 daily average trip duration (minutes) for 2024, produced by aggregating trip-level records (via PySpark) to one row per day.

\begin{enumerate}
\item \textbf{Filter to Uber trips.} Retain only records with \texttt{hvfhs\_license\_num} equal to \texttt{HV0003} (Uber).
\item \textbf{Derived columns.} Compute trip duration in minutes (\texttt{trip\_time}/60) and average speed in mph (\texttt{trip\_miles}/(\texttt{trip\_time}/3600)).
\item \textbf{Basic cleaning.} Remove trips with zero or negative duration, distance, or speed. Remove obvious outliers: trip duration outside 1--120 minutes, trip distance outside 0.1--100 miles, average speed outside 1--80 mph.
\item \textbf{IQR-based outlier removal.} For \texttt{trip\_duration\_min}, \texttt{trip\_miles}, and \texttt{avg\_speed\_mph}, compute the interquartile range (IQR) and drop trips above $Q_3 + 1.5 \times \text{IQR}$ for each variable.
\item \textbf{Aggregation.} Group by \texttt{pickup\_date} (date of \texttt{pickup\_datetime}), compute the mean trip duration per day (\texttt{avg\_duration\_min}), order by date, and round the average. The result is one row per day (366 days for 2024).
\item \textbf{Loading for modeling.} Both methods use the same CSV file containing the 366 data points (listing ~\ref{lst:load_time_series}). No additional shared preprocessing (e.g., scaling or differencing) is applied before method-specific steps. ARIMA may use differencing in the model, and LSTM uses train/validation/test splits and scaling within its pipeline.
\end{enumerate}

\subsection{Modeling Plan}

\paragraph{ARIMA.}
The first forecasting method is an ARIMA model fitted to the time-series. The ARIMA model is created using the \texttt{auto.arima} function in R. This function combines stationarity testing (Augmented Dickey--Fuller test) and chooses the best model based on AIC and BIC \cite{forecast_auto_arima}. The code is shown in listing ~\ref{lst:arima_model}.

\paragraph{LSTM.}
The second method is a long short-term memory (LSTM) network implemented in R with the \texttt{keras} package \cite{keras_r}, following time-series forecasting patterns \cite{keras_timeseries}. The series is split chronologically into train, validation, and test (e.g., last 14 days held out as test). Training data are scaled (e.g., min-max) and used to build sliding-window sequences of fixed length (e.g., 21 days) to predict the next day. The model has one LSTM layer (e.g., 32 units), dropout for regularization, and a dense output. It is trained with early stopping on validation loss. The 14-day forecast is produced recursively: predict one step ahead, append to the input sequence, and repeat. This keeps the LSTM pipeline comparable to ARIMA in horizon and evaluation.

\paragraph{Comparison.}
Both methods are evaluated on the same 14-day horizon and the same test period. Metrics are sMAPE and MASE \cite{hyndman2006,prabhat2024}. Results are reported in a single table (ARIMA vs LSTM: sMAPE, MASE) and one figure (actual vs both forecasts over the 14 days).

\subsection{Evaluation Plan}

Metrics are sMAPE and MASE over the 14-day forecast \cite{hyndman2006,prabhat2024}. MASE is scaled by the mean absolute error of a naive one-step forecast on the training data \cite{hyndman2006}. The same test period is used for both methods. Results are presented in a table (ARIMA vs LSTM: sMAPE, MASE) and a figure (actual vs both forecasts over the 14 days).

%=============================================================================
\section{Dataset and Access}
%=============================================================================

\subsection{Source}

The primary data source used in the capstone is the CSV file explained in ~\ref{sec:preprocessing}.

\subsection{Access and Loading}

The dataset is loaded in R via \texttt{read.csv()} (listing ~\ref{lst:load_time_series}) The series has \textbf{366 rows} (one per day in 2024) and \textbf{2 columns}: \texttt{pickup\_date} (Date) and \texttt{avg\_duration\_min} (numeric). There are \textbf{no missing values} in the aggregated series. The CSV is located in the capstone repo (e.g.\ \texttt{code/data/finalproject.csv} or \texttt{docs/hw4-5/data/finalproject.csv}); the same file is used by both ARIMA (in the prior R/Quarto pipeline) and LSTM (\texttt{lstm\_forecast.qmd}). Table~\ref{tab:csv_preview} shows the first five rows of the loaded data (with \texttt{weekday} derived for display).

\begin{table}[ht]
\centering
\caption{First five rows of the CSV (pickup\_date, avg\_duration\_min, weekday).}
\label{tab:csv_preview}
\begin{tabular}{lcr}
\toprule
\texttt{pickup\_date} & \texttt{avg\_duration\_min} & \texttt{weekday} \\
\midrule
2024-01-01 & 18 & Monday \\
2024-01-02 & 18 & Tuesday \\
2024-01-03 & 18 & Wednesday \\
2024-01-04 & 18 & Thursday \\
2024-01-05 & 18 & Friday \\
\bottomrule
\end{tabular}
\end{table}

%=============================================================================
\section{Implementation and Experiments}
%=============================================================================

\subsection{What Was Implemented}

\paragraph{Preprocessing (\texttt{capstone\_project\_preprocessing.ipynb}).} A PySpark Jupyter notebook loads the NYC TLC High-Volume FHV parquet data, filters to Uber trips, derives \texttt{trip\_duration\_min} and \texttt{avg\_speed\_mph}, applies basic range filters and IQR-based outlier removal, aggregates by \texttt{pickup\_date} to daily \texttt{avg\_duration\_min}, and writes the 366-day time series to CSV (see Section~\ref{sec:preprocessing} and Appendix~\ref{appendix:pyspark}).

\paragraph{ARIMA (\texttt{time\_series\_forecast.qmd}).} The R/Quarto pipeline loads \texttt{finalproject.csv} and uses the same chronological split as LSTM: last 14 days as test, preceding 30 as validation, remainder as training. It runs \texttt{auto.arima}, which selects ARIMA(3,1,2)). A 44-step forecast is produced. The last 14 steps are extracted to align with the LSTM test window. The pipeline computes sMAPE and MASE using \texttt{Metrics::smape} and \texttt{yardstick::mase\_vec}, and writes \texttt{arima\_forecast\_14day.csv} (columns: \texttt{date}, \texttt{actual}, \texttt{forecast}) to \texttt{../data/} for comparison. It also includes an optional side-by-side comparison with LSTM when \texttt{lstm\_forecast\_14day.csv} is present.

\paragraph{LSTM (\texttt{lstm\_forecast.qmd}).} The R/Quarto pipeline loads the same CSV and applies a chronological train/validation/test split (last 14 days test, preceding 30 validation). Values are min-max scaled using the training range. Sliding-window sequences of length \texttt{SEQ\_LEN = 21} are built via \texttt{create\_sequences()}; the model is a single LSTM layer (32 units), dropout (0.2), and a dense output, implemented with the Keras R API (functional API). Training uses 20\% of training sequences as validation for early stopping (\texttt{patience = 15}, \texttt{restore\_best\_weights = TRUE}), Adam optimizer, MSE loss, up to 100 epochs, batch size 16. The 14-day forecast is produced recursively: the last 21-day window before the test period is fed to the model; each predicted value is appended and the window is rolled forward. The pipeline saves \texttt{lstm\_forecast\_14day.csv}, plots training/validation loss and actual vs LSTM forecast, computes sMAPE and MASE, and optionally compares with ARIMA when \texttt{arima\_forecast\_14day.csv} exists.

\paragraph{Comparison (\texttt{compare\_forecasts.py}).} A Python script reads \texttt{lstm\_forecast\_14day.csv} and \texttt{arima\_forecast\_14day.csv} (and optionally \texttt{finalproject.csv} for the naive one-step MAE used in MASE). It implements \texttt{smape()} and \texttt{mase()}, computes both metrics for ARIMA and LSTM over the same 14-day period, prints a comparison table, writes \texttt{comparison\_table.csv}, and produces a single figure (actual vs ARIMA vs LSTM) saved as \texttt{comparison\_plot.png}. The script accepts \texttt{--data-dir} (default: \texttt{docs/hw4-5/data}) and \texttt{--out-dir} for output paths.

\subsection{What Was Tested}

Dataset: \texttt{finalproject.csv} (full 2024, 366 days). For LSTM, the series is split chronologically: the last 14 days are held out as test; the preceding 30 days as validation; the remainder as training. The same 14-day test window is used for evaluation so that ARIMA and LSTM can be compared on the same dates. Target variable: \texttt{avg\_duration\_min} (daily average trip duration in minutes).

\subsection{What Worked and What Did Not}

ARIMA fitting and 14-day forecasting run successfully with the chosen specification. The LSTM pipeline runs end-to-end; early stopping on validation loss is used to limit overfitting. Limitations encountered: (i)~small sample size (366 points) limits LSTM capacity and favors simpler models; (ii)~hyperparameters (sequence length, units, dropout) were chosen pragmatically and could be refined; (iii)~alignment of the 14-day evaluation period between ARIMA and LSTM must be ensured (e.g., both forecasting the same last-14-days window) for a fair comparison.

\subsection{Preliminary Outputs}

ARIMA: 14-day forecast table and plot are included as excerpts in Section~\ref{sec:results-arima}. LSTM: 14-day forecast figure and training/validation loss curve are produced by \texttt{lstm\_forecast.qmd}. Comparison: table (ARIMA vs LSTM sMAPE, MASE) and plot (actual vs both forecasts over the 14 days) are generated by \texttt{compare\_forecasts.py} and included in Section~\ref{sec:results-compare}.

%=============================================================================
\section{Results (Preliminary)}
%=============================================================================

\subsection{ARIMA}\label{sec:results-arima}

The ARIMA(3,1,2) model produces a 14-day point forecast with 80\% and 95\% prediction intervals. Excerpts of the forecast table (dates, point forecast, Lo/Hi 80 and 95) and the forecast plot are included here. [Insert forecast table and figure from the ARIMA pipeline run.]

\subsection{LSTM}

The LSTM model (one layer, 32 units, sequence length 21, dropout 0.2) produces a 14-day recursive forecast. Training and validation loss curves from \texttt{lstm\_forecast.qmd} show convergence; early stopping is used. sMAPE and MASE over the 14-day test window are reported in the comparison table below. [Insert LSTM forecast figure and loss curve from \texttt{lstm\_forecast.qmd}.]

\subsection{Comparison (ARIMA vs LSTM)}\label{sec:results-compare}

Table~\ref{tab:compare} summarizes sMAPE (\%) and MASE for both methods over the same 14-day period. A figure (actual vs ARIMA vs LSTM) illustrates the forecasts.

\begin{table}[ht]
\centering
\caption{Comparison of ARIMA and LSTM on 14-day forecast (sMAPE and MASE). Replace with output from \texttt{compare\_forecasts.py}.}
\label{tab:compare}
\begin{tabular}{lcc}
\toprule
Model & sMAPE (\%) & MASE \\
\midrule
ARIMA & -- & -- \\
LSTM  & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

[Insert comparison figure from \texttt{compare\_forecasts.py}.] Interpretation: [after running both pipelines and the comparison script, state which method performed better on sMAPE and MASE and briefly discuss possible reasons (e.g., ARIMA stability with small data vs LSTM flexibility).]

%=============================================================================
\section{Issues and Limitations}
%=============================================================================

Limitations include: (i)~\textbf{Small sample size}---366 daily observations is on the small side for deep learning; the LSTM has limited training sequences after the chronological split, which may favor the simpler ARIMA model. (ii)~\textbf{Univariate models}---both methods use only the daily average trip duration; no exogenous regressors (e.g., day-of-week, holidays) are included. (iii)~\textbf{Alignment of evaluation period}---for a fair comparison, ARIMA and LSTM must forecast the same 14-day window (e.g., last 14 days of 2024); if ARIMA was run for a different horizon (e.g., 2025-01-01 onward), the comparison table and figure should use aligned dates. (iv)~Any bugs or data issues encountered during implementation can be noted here.

%=============================================================================
\section{Next Steps}
%=============================================================================

Complete the comparison script run and insert the final comparison table and figure into the report; add the ARIMA 14-day forecast to the comparison if not yet exported. If more years of NYC TLC data become available, extend the series to improve LSTM training. Refine LSTM hyperparameters (sequence length, units, dropout) and consider cross-validation. Finalize the report (replace placeholders with actual tables and figures), export to PDF, and submit.

%=============================================================================
\appendix
%=============================================================================

\section{PySpark Preprocessing Code}\label{appendix:pyspark}

The following PySpark script was used to preprocess the NYC TLC High-Volume FHV trip record data and compute the average daily trip durations that feed the time series (see Section~\ref{sec:preprocessing}). The code is taken from \texttt{code/capstone\_project\_preprocessing.ipynb}.

First, PySpark is installed, a Spark session is created, and the parquet data are loaded (Listing~\ref{lst:pyspark_load}).

\begin{lstlisting}[style=pystyle, caption={Spark session and loading FHV data from parquet}, label=lst:pyspark_load]
!pip install pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").appName("capstone-project").getOrCreate()

data_directory = "/content/drive/MyDrive/datasets/nyc-taxi/"
df = spark.read.format("parquet").option("recursiveFileLookup", "true").load(data_directory)
print(f"Row count: {df.count()}")
\end{lstlisting}

Only Uber trips are retained (Listing~\ref{lst:pyspark_uber}).

\begin{lstlisting}[style=pystyle, caption={Filter to Uber trips}, label=lst:pyspark_uber]
df = df.filter(df["hvfhs_license_num"] == "HV0003")
print(f"Uber trip count: {df.count()}")
\end{lstlisting}

Derived columns are added for trip duration in minutes and average speed (Listing~\ref{lst:pyspark_derived_cols}).

\begin{lstlisting}[style=pystyle, caption={Deriving trip duration and average speed columns}, label=lst:pyspark_derived_cols]
from pyspark.sql.functions import col

df = df.withColumn("trip_duration_min", col("trip_time") / 60)
df = df.withColumn("avg_speed_mph", (col("trip_miles") / (col("trip_time") / 3600)))
\end{lstlisting}

The data are cleaned (Listing~\ref{lst:pyspark_cleaning}): trips with zero or negative duration, distance, or speed are removed; then trips outside the ranges 1--120 minutes, 0.1--100 miles, and 1--80 mph are removed.

\begin{lstlisting}[style=pystyle, caption={Data cleaning}, label=lst:pyspark_cleaning]
df_clean = df.filter(
    (col("trip_duration_min") > 0) &
    (col("trip_miles") > 0) &
    (col("avg_speed_mph") > 0)
)

df_clean = df_clean.filter(
    (col("trip_duration_min") <= 120) &
    (col("trip_duration_min") >= 1) &
    (col("trip_miles") <= 100) &
    (col("trip_miles") >= 0.1) &
    (col("avg_speed_mph") <= 80) &
    (col("avg_speed_mph") >= 1)
)
\end{lstlisting}

IQR-based outlier removal is applied (Listing~\ref{lst:pyspark_outliers}): for each of \texttt{trip\_duration\_min}, \texttt{trip\_miles}, and \texttt{avg\_speed\_mph}, trips above $Q_3 + 1.5 \times \text{IQR}$ are dropped.

\begin{lstlisting}[style=pystyle, caption={IQR-based outlier removal}, label=lst:pyspark_outliers]
columns_to_check = ["trip_duration_min", "trip_miles", "avg_speed_mph"]
iqr_dict = {}

for col_name in columns_to_check:
    q1, q3 = df_clean.approxQuantile(col_name, [0.25, 0.75], 0.01)
    iqr = q3 - q1
    iqr_dict[col_name] = (q1, q3, iqr)
    print(f"IQR for {col_name}: {iqr_dict[col_name][2]}")
    print(f"  Q1 for {col_name}: {iqr_dict[col_name][0]}")
    print(f"  Q3 for {col_name}: {iqr_dict[col_name][1]}")

iqr_duration = iqr_dict["trip_duration_min"][2]
iqr_miles = iqr_dict["trip_miles"][2]
iqr_speed = iqr_dict["avg_speed_mph"][2]

iqr_factor = 1.5

iqr_duration_15 = iqr_factor * iqr_duration
iqr_miles_15 = iqr_factor * iqr_miles
iqr_speed_15 = iqr_factor * iqr_speed

duration_upper = iqr_dict["trip_duration_min"][1] + iqr_duration_15
miles_upper = iqr_dict["trip_miles"][1] + iqr_miles_15
speed_upper = iqr_dict["avg_speed_mph"][1] + iqr_speed_15

df_final = df_clean.filter(
    (col("trip_duration_min") <= duration_upper) &
    (col("trip_miles") <= miles_upper) &
    (col("avg_speed_mph") <= speed_upper)
)
\end{lstlisting}

The \texttt{pickup\_date} column is added and the average trip duration per day is computed on the cleaned Uber data (Listing~\ref{lst:pyspark_avg_duration}).

\begin{lstlisting}[style=pystyle, caption={Add pickup date and calculate average daily duration}, label=lst:pyspark_avg_duration]
from pyspark.sql.functions import to_date

df_final = df_final.withColumn("pickup_date", to_date(col("pickup_datetime")))

from pyspark.sql.functions import avg, round

df_daily_avg = df_final.groupBy("pickup_date") \
                   .agg(avg("trip_duration_min").alias("avg_duration_min")) \
                   .orderBy("pickup_date")

df_daily_avg = df_daily_avg.withColumn("avg_duration_min", round("avg_duration_min"))
\end{lstlisting}

Finally, the time series is written to a single CSV file (Listing~\ref{lst:pyspark_csv}) with two columns: \texttt{pickup\_date} and \texttt{avg\_duration\_min}.

\begin{lstlisting}[style=pystyle, caption={Saving time series to CSV}, label=lst:pyspark_csv]
final_path = "/content/drive/MyDrive/datasets/fhvhv_tripdata_2024_uber_timeseries.csv"

df_daily_avg.select("pickup_date", "avg_duration_min") \
         .coalesce(1) \
         .write.option("header", True) \
         .mode("overwrite") \
         .csv(final_path)
\end{lstlisting}

\section{Time-series Modeling Code}\label{appendix:time_series_modeling_code}
\begin{lstlisting}[style=pystyle, caption={Loading Time Series}, label=lst:load_time_series]
data <- read.csv(file.path(getwd(), "../data", "finalproject.csv"))
ts_data <- ts(data$avg_duration_min, frequency = 366, start = c(2024, 1))
\end{lstlisting}


The \texttt{auto.arima} function is used to determine the best time-series model based on AIC and BIC.

\begin{lstlisting}[style=pystyle, caption={ARIMA Model}, label=lst:arima_model]
library(forecast)
fit_auto <- auto.arima(ts_data)
\end{lstlisting}


%=============================================================================
\bibliographystyle{plain}
\bibliography{week4_5_report}
%=============================================================================
\end{document}
